{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a8b376",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9412f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f148fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase output limit\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Or right-click on output â†’ \"Create New View for Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bd96e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Base', 'Variant I', 'Variant II', 'Variant III', 'Variant IV', 'Variant V']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob, pandas as pd\n",
    "\n",
    "RAW_DIR = \"data/\"   # update to actual path\n",
    "EXT = \"csv\"                # or \"parquet\"\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(RAW_DIR, f\"*.{EXT}\")))\n",
    "def load_one(path):\n",
    "    return pd.read_parquet(path) if path.endswith(\".parquet\") else pd.read_csv(path)\n",
    "\n",
    "def variant_name(path):\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "variants = {variant_name(p): load_one(p) for p in files}\n",
    "list(variants.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181cf643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def infer_label_col(df):\n",
    "    candidates = [c for c in df.columns if any(k in c.lower() for k in [\"fraud\",\"label\",\"target\",\"is_fraud\"])]\n",
    "    for c in candidates:\n",
    "        u = sorted(df[c].dropna().unique())\n",
    "        if len(u) <= 3 and set(u).issubset({0,1}):\n",
    "            return c\n",
    "    # Fallback: find any binary-int column\n",
    "    for c in df.columns:\n",
    "        u = sorted(df[c].dropna().unique())\n",
    "        if len(u) <= 3 and set(u).issubset({0,1}):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def infer_time_col(df):\n",
    "    # already datetime?\n",
    "    dt_candidates = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "    if dt_candidates:\n",
    "        return dt_candidates[0]\n",
    "    # try parsing common names\n",
    "    name_hits = [c for c in df.columns if any(k in c.lower() for k in [\"time\",\"date\",\"timestamp\",\"dt\"])]\n",
    "    for c in name_hits:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n",
    "            df[c] = parsed\n",
    "            return c\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def candidate_groups(df, max_card=50):\n",
    "    cats = []\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\" or str(df[c].dtype) == \"category\":\n",
    "            if df[c].nunique(dropna=True) <= max_card:\n",
    "                cats.append(c)\n",
    "    return cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8e5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13344\\2178368914.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: Base\n",
      "Rows, Cols: (1000000, 32)\n",
      "Label: fraud_bool\n",
      "Time: date_of_birth_distinct_emails_4w\n",
      "Group candidates: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
      "Null % (top 10):\n",
      "fraud_bool                   0.0\n",
      "income                       0.0\n",
      "device_fraud_count           0.0\n",
      "device_distinct_emails_8w    0.0\n",
      "keep_alive_session           0.0\n",
      "device_os                    0.0\n",
      "session_length_in_minutes    0.0\n",
      "source                       0.0\n",
      "foreign_request              0.0\n",
      "proposed_credit_limit        0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13344\\2178368914.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: Variant I\n",
      "Rows, Cols: (1000000, 32)\n",
      "Label: fraud_bool\n",
      "Time: date_of_birth_distinct_emails_4w\n",
      "Group candidates: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
      "Null % (top 10):\n",
      "fraud_bool                   0.0\n",
      "income                       0.0\n",
      "device_fraud_count           0.0\n",
      "device_distinct_emails_8w    0.0\n",
      "keep_alive_session           0.0\n",
      "device_os                    0.0\n",
      "session_length_in_minutes    0.0\n",
      "source                       0.0\n",
      "foreign_request              0.0\n",
      "proposed_credit_limit        0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13344\\2178368914.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: Variant II\n",
      "Rows, Cols: (1000000, 32)\n",
      "Label: fraud_bool\n",
      "Time: date_of_birth_distinct_emails_4w\n",
      "Group candidates: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
      "Null % (top 10):\n",
      "fraud_bool                   0.0\n",
      "income                       0.0\n",
      "device_fraud_count           0.0\n",
      "device_distinct_emails_8w    0.0\n",
      "keep_alive_session           0.0\n",
      "device_os                    0.0\n",
      "session_length_in_minutes    0.0\n",
      "source                       0.0\n",
      "foreign_request              0.0\n",
      "proposed_credit_limit        0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13344\\2178368914.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: Variant III\n",
      "Rows, Cols: (1000000, 34)\n",
      "Label: fraud_bool\n",
      "Time: date_of_birth_distinct_emails_4w\n",
      "Group candidates: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
      "Null % (top 10):\n",
      "fraud_bool                   0.0\n",
      "source                       0.0\n",
      "phone_home_valid             0.0\n",
      "phone_mobile_valid           0.0\n",
      "bank_months_count            0.0\n",
      "has_other_cards              0.0\n",
      "proposed_credit_limit        0.0\n",
      "foreign_request              0.0\n",
      "session_length_in_minutes    0.0\n",
      "income                       0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13344\\2178368914.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: Variant IV\n",
      "Rows, Cols: (1000000, 32)\n",
      "Label: fraud_bool\n",
      "Time: date_of_birth_distinct_emails_4w\n",
      "Group candidates: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
      "Null % (top 10):\n",
      "fraud_bool                   0.0\n",
      "income                       0.0\n",
      "device_fraud_count           0.0\n",
      "device_distinct_emails_8w    0.0\n",
      "keep_alive_session           0.0\n",
      "device_os                    0.0\n",
      "session_length_in_minutes    0.0\n",
      "source                       0.0\n",
      "foreign_request              0.0\n",
      "proposed_credit_limit        0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13344\\2178368914.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[c], errors=\"raise\", utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: Variant V\n",
      "Rows, Cols: (1000000, 34)\n",
      "Label: fraud_bool\n",
      "Time: date_of_birth_distinct_emails_4w\n",
      "Group candidates: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
      "Null % (top 10):\n",
      "fraud_bool                   0.0\n",
      "source                       0.0\n",
      "phone_home_valid             0.0\n",
      "phone_mobile_valid           0.0\n",
      "bank_months_count            0.0\n",
      "has_other_cards              0.0\n",
      "proposed_credit_limit        0.0\n",
      "foreign_request              0.0\n",
      "session_length_in_minutes    0.0\n",
      "income                       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def schema_report(name, df):\n",
    "    label = infer_label_col(df)\n",
    "    timec = infer_time_col(df)\n",
    "    groups = candidate_groups(df)\n",
    "\n",
    "    print(f\"Variant: {name}\")\n",
    "    print(\"Rows, Cols:\", df.shape)\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Time:\", timec)\n",
    "    print(\"Group candidates:\", groups[:10])\n",
    "    print(\"Null % (top 10):\")\n",
    "    print((df.isna().mean().sort_values(ascending=False).head(10) * 100).round(2))\n",
    "\n",
    "for name, df in variants.items():\n",
    "    schema_report(name, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4275cf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011029,\n",
       "                                   pos_rate\n",
       " date_of_birth_distinct_emails_4w          \n",
       " 1970-01-31 00:00:00+00:00         0.011029)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prevalence_summary(df, label_col, time_col=None, freq=\"M\"):\n",
    "    pr = df[label_col].mean()\n",
    "    out = {\"overall_pos_rate\": pr}\n",
    "    if time_col:\n",
    "        by_time = df.set_index(time_col).resample(freq)[label_col].mean().rename(\"pos_rate\").to_frame()\n",
    "        out[\"by_time\"] = by_time\n",
    "    return out\n",
    "\n",
    "summaries = {}\n",
    "for name, df in variants.items():\n",
    "    ycol = infer_label_col(df)\n",
    "    tcol = infer_time_col(df)\n",
    "    summaries[name] = prevalence_summary(df, ycol, tcol)\n",
    "\n",
    "# Example: inspect one\n",
    "name0 = list(summaries.keys())[0]\n",
    "summaries[name0][\"overall_pos_rate\"], summaries[name0].get(\"by_time\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24eeab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base {'min_ts': Timestamp('1970-01-01 00:00:00+0000', tz='UTC'), 'max_ts': Timestamp('1970-01-01 00:00:00.000000039+0000', tz='UTC'), 'n_dupe_ts': 999960, 'rows_w_time': 1000000}\n",
      "Variant I {'min_ts': Timestamp('1970-01-01 00:00:00+0000', tz='UTC'), 'max_ts': Timestamp('1970-01-01 00:00:00.000000039+0000', tz='UTC'), 'n_dupe_ts': 999960, 'rows_w_time': 1000000}\n",
      "Variant II {'min_ts': Timestamp('1970-01-01 00:00:00+0000', tz='UTC'), 'max_ts': Timestamp('1970-01-01 00:00:00.000000039+0000', tz='UTC'), 'n_dupe_ts': 999960, 'rows_w_time': 1000000}\n",
      "Variant III {'min_ts': Timestamp('1970-01-01 00:00:00+0000', tz='UTC'), 'max_ts': Timestamp('1970-01-01 00:00:00.000000039+0000', tz='UTC'), 'n_dupe_ts': 999960, 'rows_w_time': 1000000}\n",
      "Variant IV {'min_ts': Timestamp('1970-01-01 00:00:00+0000', tz='UTC'), 'max_ts': Timestamp('1970-01-01 00:00:00.000000039+0000', tz='UTC'), 'n_dupe_ts': 999960, 'rows_w_time': 1000000}\n",
      "Variant V {'min_ts': Timestamp('1970-01-01 00:00:00+0000', tz='UTC'), 'max_ts': Timestamp('1970-01-01 00:00:00.000000037+0000', tz='UTC'), 'n_dupe_ts': 999962, 'rows_w_time': 1000000}\n"
     ]
    }
   ],
   "source": [
    "def time_checks(df, time_col):\n",
    "    dft = df.dropna(subset=[time_col]).sort_values(time_col)\n",
    "    return {\n",
    "        \"min_ts\": dft[time_col].min(),\n",
    "        \"max_ts\": dft[time_col].max(),\n",
    "        \"n_dupe_ts\": dft.duplicated(subset=[time_col]).sum(),\n",
    "        \"rows_w_time\": len(dft),\n",
    "    }\n",
    "\n",
    "for name, df in variants.items():\n",
    "    tcol = infer_time_col(df)\n",
    "    if tcol:\n",
    "        print(name, time_checks(df, tcol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40dbb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base group prevalence (top 2 candidates):\n",
      "payment_type\n",
      "               count      mean\n",
      "payment_type                  \n",
      "AB            370554  0.011251\n",
      "AA            258249  0.005282\n",
      "AC            252071  0.016698\n",
      "AD            118837  0.010822\n",
      "AE               289  0.003460\n",
      "employment_status\n",
      "                    count      mean\n",
      "employment_status                  \n",
      "CA                 730252  0.012186\n",
      "CB                 138288  0.006891\n",
      "CF                  44034  0.001930\n",
      "CC                  37758  0.024684\n",
      "CD                  26522  0.003770\n",
      "CE                  22693  0.002336\n",
      "CG                    453  0.015453\n",
      "\n",
      "Variant I group prevalence (top 2 candidates):\n",
      "payment_type\n",
      "               count      mean\n",
      "payment_type                  \n",
      "AB            364724  0.011060\n",
      "AA            260651  0.005417\n",
      "AC            253419  0.016976\n",
      "AD            120964  0.010582\n",
      "AE               242  0.004132\n",
      "employment_status\n",
      "                    count      mean\n",
      "employment_status                  \n",
      "CA                 742492  0.012674\n",
      "CB                 137901  0.007114\n",
      "CF                  43573  0.001997\n",
      "CD                  26232  0.004308\n",
      "CC                  25806  0.014493\n",
      "CE                  23562  0.002419\n",
      "CG                    434  0.016129\n",
      "\n",
      "Variant II group prevalence (top 2 candidates):\n",
      "payment_type\n",
      "               count      mean\n",
      "payment_type                  \n",
      "AB            395277  0.011212\n",
      "AA            251688  0.005400\n",
      "AC            242178  0.016773\n",
      "AD            110604  0.010633\n",
      "AE               253  0.000000\n",
      "employment_status\n",
      "                    count      mean\n",
      "employment_status                  \n",
      "CA                 677869  0.012038\n",
      "CB                 141784  0.006411\n",
      "CC                  88937  0.019744\n",
      "CF                  44833  0.001517\n",
      "CD                  27328  0.003440\n",
      "CE                  18758  0.001972\n",
      "CG                    491  0.010183\n",
      "\n",
      "Variant III group prevalence (top 2 candidates):\n",
      "payment_type\n",
      "               count      mean\n",
      "payment_type                  \n",
      "AB            399268  0.010577\n",
      "AA            249278  0.005416\n",
      "AC            247027  0.017443\n",
      "AD            104190  0.011018\n",
      "AE               237  0.000000\n",
      "employment_status\n",
      "                    count      mean\n",
      "employment_status                  \n",
      "CA                 684981  0.012827\n",
      "CB                 135133  0.006564\n",
      "CC                  89089  0.012830\n",
      "CF                  44943  0.001402\n",
      "CD                  26648  0.003640\n",
      "CE                  18719  0.002564\n",
      "CG                    487  0.012320\n",
      "\n",
      "Variant IV group prevalence (top 2 candidates):\n",
      "payment_type\n",
      "               count      mean\n",
      "payment_type                  \n",
      "AB            399231  0.011081\n",
      "AA            249256  0.005179\n",
      "AC            246978  0.016973\n",
      "AD            104298  0.010767\n",
      "AE               237  0.000000\n",
      "employment_status\n",
      "                    count      mean\n",
      "employment_status                  \n",
      "CA                 685029  0.012194\n",
      "CB                 135022  0.006177\n",
      "CC                  89145  0.018431\n",
      "CF                  44949  0.001424\n",
      "CD                  26626  0.003230\n",
      "CE                  18745  0.002347\n",
      "CG                    484  0.012397\n",
      "\n",
      "Variant V group prevalence (top 2 candidates):\n",
      "payment_type\n",
      "               count      mean\n",
      "payment_type                  \n",
      "AB            399073  0.010497\n",
      "AA            249581  0.005385\n",
      "AC            246596  0.017441\n",
      "AD            104489  0.011408\n",
      "AE               261  0.015326\n",
      "employment_status\n",
      "                    count      mean\n",
      "employment_status                  \n",
      "CA                 685037  0.012877\n",
      "CB                 134222  0.006534\n",
      "CC                  89437  0.012500\n",
      "CF                  45225  0.001813\n",
      "CD                  26963  0.002856\n",
      "CE                  18653  0.002681\n",
      "CG                    463  0.010799\n"
     ]
    }
   ],
   "source": [
    "def group_prevalence(df, label_col, group_col, min_size=200):\n",
    "    grp = df.groupby(group_col)[label_col].agg([\"count\",\"mean\"]).sort_values(\"count\", ascending=False)\n",
    "    return grp[grp[\"count\"] >= min_size]\n",
    "\n",
    "for name, df in variants.items():\n",
    "    ycol = infer_label_col(df)\n",
    "    gcols = candidate_groups(df)\n",
    "    print(f\"\\n{name} group prevalence (top 2 candidates):\")\n",
    "    for gc in gcols[:2]:\n",
    "        print(gc)\n",
    "        print(group_prevalence(df, ycol, gc).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa87fd74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m out \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROC_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mdfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrote:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out, dfx\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2891\u001b[0m     path,\n\u001b[0;32m   2892\u001b[0m     engine,\n\u001b[0;32m   2893\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   2894\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   2895\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   2896\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2897\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2898\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parquet.py:407\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    406\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 407\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m    411\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    412\u001b[0m     df,\n\u001b[0;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    419\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     58\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "PROC_DIR = \"data/baf_processed\"\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "\n",
    "def to_processed(df, label_col, time_col, drop_cols=None):\n",
    "    dfx = df.copy()\n",
    "    if time_col and not pd.api.types.is_datetime64_any_dtype(dfx[time_col]):\n",
    "        dfx[time_col] = pd.to_datetime(dfx[time_col], errors=\"coerce\", utc=True, infer_datetime_format=True)\n",
    "    dfx[label_col] = dfx[label_col].astype(\"int8\")\n",
    "    # Drop obvious IDs/leakage if known\n",
    "    if drop_cols:\n",
    "        dfx = dfx.drop(columns=[c for c in drop_cols if c in dfx.columns])\n",
    "    # Cast object low-cardinality to category for memory efficiency\n",
    "    for c in dfx.columns:\n",
    "        if dfx[c].dtype == \"object\" and dfx[c].nunique(dropna=True) <= 1000:\n",
    "            dfx[c] = dfx[c].astype(\"category\")\n",
    "    return dfx\n",
    "\n",
    "for name, df in variants.items():\n",
    "    ycol = infer_label_col(df)\n",
    "    tcol = infer_time_col(df)\n",
    "    dfx = to_processed(df, ycol, tcol, drop_cols=None)  # provide known IDs if any\n",
    "    ext = \"parquet\"\n",
    "    out = os.path.join(PROC_DIR, f\"{name}.parquet\")\n",
    "    dfx.to_parquet(out, index=False)\n",
    "    print(\"Wrote:\", out, dfx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2a7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
